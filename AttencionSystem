local AttentionNN = {}
AttentionNN.Weights =    {
	hidden1 = {
		[1] = {-0.36280,-1.16910,0.34376,-0.66260,0.70785,0.76604,-0.00562,-0.08676,0.14602,0.64053,0.65516,-0.62884},
		[2] = {-0.13034,0.01859,-0.15337,-0.19920,-0.25490,-0.17866,0.15267,-0.25345,0.11672,-0.12901,0.08624,0.15793},
		[3] = {-0.44831,1.34927,0.48990,-2.99155,0.15781,-0.08416,-0.24752,0.13335,-0.12738,-0.18787,0.65337,0.62546},
		[4] = {-0.15734,-0.38306,-0.43226,0.46753,-0.75848,0.22777,0.18014,0.22353,-0.13378,-0.26741,1.71366,-0.35754},
		[5] = {-1.22904,-0.47232,1.58548,1.74117,1.05524,0.05263,-0.15285,0.20585,0.24170,-0.27920,-0.50760,-0.66992},
		[6] = {0.28435,-0.17231,-0.08950,-0.11706,0.19619,-0.21257,-0.16006,0.26749,0.21096,-0.01858,-0.03403,-0.20045},
		[7] = {-0.08863,0.03212,0.17054,-0.24002,-0.18390,0.04721,0.03781,-0.19697,0.10252,-0.10958,-0.33147,0.18658},
		[8] = {0.30495,-0.17473,0.49142,0.42353,0.50954,0.07526,0.16908,-0.02054,0.07114,-0.23425,-1.02632,-0.09899},
		[9] = {-0.08963,-0.31004,-0.08984,0.19460,0.11821,0.03756,-0.08749,0.10196,-0.16404,-0.22543,-0.18233,-0.04873},
		[10] = {-0.13869,-0.22933,0.06135,0.07797,0.18552,-0.20637,-0.05577,-0.22392,-0.20484,-0.06096,0.02603,0.05237},
		[11] = {1.74099,0.39226,-1.26545,0.25207,-1.54431,-0.40295,-0.00922,-0.06269,-0.06174,-0.25019,0.32642,0.67797},
		[12] = {-0.26494,-0.34285,0.20928,0.09948,0.08288,-0.28347,0.22828,-0.21008,-0.17235,0.26498,-0.39163,-0.22961},
		[13] = {-0.02038,-0.46830,-0.67218,-0.41990,0.68721,1.52256,0.00059,-0.00327,0.40883,0.08329,-0.72140,0.53686},
		[14] = {0.29114,0.97305,-2.02100,-0.50445,2.44235,-1.40208,-0.28948,0.28319,-0.26775,0.47580,-0.73052,0.53356},
		[15] = {0.08278,-0.07404,-0.72692,0.15584,-0.25210,1.26869,0.02954,0.26926,-0.07852,1.04888,-1.45877,-0.10073},
		[16] = {-0.09342,-0.00480,-1.22111,1.11798,0.62744,0.41364,0.00211,0.13048,0.16952,-0.43479,0.41714,-0.63852},
		[17] = {-0.50739,0.62524,0.07278,-0.71002,-1.32067,0.62698,0.03907,0.27547,0.02753,-0.54608,1.06001,-0.15740},
		[18] = {-0.18671,-1.30565,-0.67840,0.61090,0.03002,-0.52158,-0.21166,0.19465,-0.06019,1.02083,0.75616,-0.78124},
		[19] = {0.76857,0.00966,-1.58889,0.18765,-0.38644,0.91625,-0.27184,0.04009,-0.29586,0.72708,-2.30879,0.63099},
		[20] = {-0.11407,0.08003,-0.22113,0.14551,-0.03530,0.02147,-0.25968,-0.05444,0.18157,-0.00463,-0.27623,-0.05233},
		[21] = {-0.27659,0.24133,1.05459,0.29583,-0.57471,-0.59042,0.22380,-0.21138,-0.30765,-0.52039,1.29541,0.58811},
		[22] = {-0.60760,0.26973,1.08162,0.19145,-0.05828,1.12112,0.36469,-0.08412,0.25416,0.27812,-1.48467,-0.58107},
		[23] = {0.03685,0.05877,-0.28530,-0.03193,0.07785,-0.24938,0.09734,0.22343,-0.26580,-0.03110,-0.21148,0.05127},
		[24] = {-0.73065,1.17679,0.13162,-1.60573,0.50461,0.68131,-0.12959,0.09839,-0.21730,-0.29697,-0.09626,0.39195},
		[25] = {-0.13908,-0.28711,0.92608,1.06508,0.45354,-0.38606,-0.12901,0.22699,-0.26875,-0.35762,-1.21188,-0.24621},
		[26] = {0.02571,-0.38445,-0.71031,-2.20584,1.47642,1.76111,-0.21600,-0.13890,-0.01015,0.55811,0.21623,-0.70390},
		[27] = {0.46848,0.34613,-2.98062,1.24003,-0.11834,1.51566,-0.40294,-0.12740,-0.42361,-0.34297,0.56443,-0.09240},
		[28] = {-0.26478,-0.13003,-0.27792,0.02195,0.16818,0.04101,-0.07548,-0.26427,0.22030,-0.03869,-0.20682,-0.15053},
		[29] = {0.76023,-0.38396,-1.86121,1.67544,-0.00711,0.40125,0.02792,-0.07643,-0.10551,0.15439,0.11505,-0.16239},
		[30] = {-0.09511,-0.29748,-0.17379,-0.09490,0.08347,-0.16465,-0.00330,-0.23717,-0.17698,-0.15559,-0.15948,0.11655},
		[31] = {0.10628,0.20180,1.14835,1.33282,-0.95929,-1.35996,0.32865,0.05848,0.22310,0.38836,-0.99826,0.37797},
		[32] = {-0.22757,-0.80246,-0.02390,-0.77370,1.40717,0.25363,-0.20398,-0.14893,0.12150,0.65324,0.03533,-0.27751},
		[33] = {0.15498,1.09379,0.23536,-1.04485,-1.08692,-0.52098,-0.06767,0.09944,-0.33712,0.53285,-0.21969,0.44012},
		[34] = {0.01780,0.29813,-0.17662,0.65199,-0.84294,0.23497,-0.06797,0.13255,-0.12530,-0.41301,0.43054,-0.36450},
		[35] = {-0.17940,-0.06821,0.06562,-0.14866,-0.17343,-0.13312,-0.24416,0.08599,-0.06493,0.11572,-0.21230,-0.16333},
		[36] = {-0.36482,-0.21221,-0.48286,-0.27215,-0.64796,-0.31214,0.18150,0.25190,0.32235,1.69554,-2.97041,-0.32613},
		[37] = {0.18154,0.25826,-1.25894,0.37536,1.31701,-0.60873,0.04405,-0.16500,-0.43954,0.24166,-0.17781,0.11711},
		[38] = {2.12022,-0.42493,-2.76460,-0.63150,0.40251,-0.77680,0.20419,-0.27999,-0.23918,-0.82167,0.69491,0.28992},
		[39] = {-0.14231,-0.07594,-0.10005,-0.08172,0.07275,-0.12149,-0.01859,0.15942,0.05440,-0.19292,-0.17188,-0.29926},
		[40] = {-0.58651,0.07972,-0.76437,-0.42763,-0.86811,-0.55454,0.28819,-0.02192,0.04177,0.17468,2.03084,-0.62602},
		[41] = {-0.21779,0.11070,0.92981,-0.83130,-0.70129,0.64556,-0.15155,-0.23626,-0.15606,0.68111,-0.68825,0.13579},
		[42] = {-0.12569,-0.21850,0.10195,0.02484,0.07251,-0.02072,-0.26417,0.00227,-0.16915,-0.10258,-0.10069,0.10159},
		[43] = {-0.26221,-0.63096,1.35487,1.82180,-0.05304,0.14587,-0.19017,0.06439,-0.20271,-0.52735,-1.76341,-0.19637},
		[44] = {-0.83901,0.62605,-0.22830,-0.95642,1.03260,-0.07940,0.05427,-0.24298,0.13328,-0.39659,0.50478,-0.02942},
		[45] = {0.48069,-0.75106,0.34381,-0.40796,-0.31726,-2.09744,0.04287,0.25100,0.19788,-0.27216,1.96908,-0.37815},
		[46] = {-0.35981,0.02328,-0.36614,1.29811,-0.26797,-1.09935,0.12406,0.04783,0.25987,0.38495,0.21861,0.18991},
		[47] = {-0.56017,-0.29652,0.82262,1.16479,0.46458,-0.12088,-0.19219,-0.04634,-0.32439,-0.71387,0.47808,-0.17091},
		[48] = {-0.07073,-0.85867,0.30924,2.98497,-0.96429,1.47201,-0.23570,0.24145,-0.24617,-0.39877,0.17027,-1.26424},
	},
	hidden2 = {
		[1] = {0.25547,0.04396,-0.01898,0.57418,0.22254,-0.00713,0.12872,0.38788,0.01840,0.01361,-0.46126,0.15550,0.03320,-0.78300,0.93813,0.01766,-0.24724,-0.14227,0.91550,0.13260,-1.03253,-0.09184,0.08528,-0.54221,-0.32478,-0.60447,0.62324,-0.08873,0.22047,0.13612,0.27319,-0.11670,0.03831,0.10930,0.00930,-0.57956,-0.55674,0.02370,0.11057,0.41459,0.21458,0.13360,-0.18803,-0.24362,0.09427,0.44915,-0.29059,0.89824},
		[2] = {0.00337,0.07509,0.21816,-0.37842,0.37847,0.04623,0.09154,0.06677,0.06409,0.09475,-0.10354,-0.00120,-0.44044,-0.62480,-0.94841,-0.06042,-0.22269,-0.09372,-0.62174,0.11475,0.61348,-0.25787,0.03406,0.27505,0.39927,0.40886,-0.61096,0.04845,-0.71909,-0.10738,-0.00801,0.17521,-0.68418,0.08925,-0.13665,-0.05190,-0.16366,-0.77560,-0.13236,0.26601,-0.58932,-0.04898,0.68199,0.05950,-0.05271,-0.13230,0.57076,-0.12606},
		[3] = {-0.22553,0.09101,-0.32290,-0.02233,-0.15174,-0.01311,-0.04990,-0.05039,-0.07394,-0.08087,0.20882,0.08307,-0.10086,-0.03517,0.19528,0.17112,0.07909,0.29658,-0.19550,-0.03619,0.16184,-0.31030,-0.13541,-0.35170,0.21574,0.06136,-0.01632,-0.00580,0.31304,-0.12204,0.35617,-0.02172,-0.02662,0.20513,0.12498,0.42121,0.04564,-0.14531,-0.08947,-0.25119,0.17459,-0.13135,0.26558,-0.05322,-0.36359,0.22024,0.08813,-0.00648},
		[4] = {-0.35202,-0.05759,-0.86324,0.00315,-0.43499,-0.10933,-0.02083,0.00570,-0.08255,0.02198,-0.16436,0.03825,-0.14086,0.48438,0.13660,0.19175,0.00216,0.05163,-0.15313,-0.12752,0.16039,-0.42018,-0.14027,-0.07918,0.03845,0.26886,-0.07377,0.11173,0.24244,0.04034,0.36467,0.16608,-0.09140,0.17070,-0.01338,0.43839,0.31252,-0.13989,-0.14177,-0.85657,0.58120,0.09116,0.32988,-0.24845,-0.18921,-0.19289,0.50296,0.00895},
		[5] = {0.04007,0.01466,0.10658,0.18628,0.10918,0.09814,-0.14207,-0.19217,0.05565,-0.04603,0.22965,-0.09901,-0.27479,-0.50974,-0.10206,0.02903,0.16857,0.05026,0.08961,-0.06568,0.15273,-0.24838,-0.00250,-0.01630,0.07878,0.16259,-0.43536,-0.09069,-0.45147,0.00033,-0.39794,0.01799,-0.33171,0.06455,0.03255,-0.05381,-0.40422,-0.46541,-0.12989,-0.03956,-0.26123,-0.10565,0.01270,0.04603,-0.22523,0.11448,0.15792,-0.04722},
		[6] = {-0.12095,0.08625,-0.04082,-0.05178,0.00065,0.04093,-0.09909,-0.03499,0.04732,-0.11934,0.04681,-0.06739,-0.11549,-0.12350,-0.10792,0.02882,0.05624,-0.07323,0.00184,-0.00011,0.00311,-0.11720,-0.12779,-0.18329,-0.13788,-0.08736,0.06673,-0.13883,0.03006,0.03650,-0.05768,-0.13035,-0.08738,-0.17529,-0.06184,-0.09170,0.02570,0.18722,-0.12185,-0.02221,-0.03334,-0.11312,-0.10944,-0.13024,-0.06674,-0.00191,-0.18745,-0.15571},
		[7] = {0.05941,-0.10760,-0.15479,0.08464,0.23203,-0.10456,0.10473,0.00052,-0.00813,-0.11086,-0.26754,-0.00302,-0.02171,0.59856,0.02501,0.54536,-0.04640,-0.06218,0.11267,0.03572,-1.05951,-0.51194,-0.00119,-0.80268,0.64802,-0.92871,-0.06275,-0.07022,0.34568,0.12621,0.07011,-0.26629,-0.40275,-0.27116,0.04434,0.48029,0.21374,-0.00133,0.14666,0.01174,-0.30063,0.01966,0.77764,-0.21202,0.10714,-0.05074,-0.01500,0.13435},
		[8] = {-0.20339,0.01518,-0.11711,-0.04251,0.10563,-0.01938,-0.04331,-0.04105,0.11130,-0.08376,-0.01669,-0.13647,0.13398,-0.02110,-0.02274,-0.02969,0.05488,-0.06164,0.08475,0.12763,0.06987,-0.01151,-0.12833,-0.05946,-0.10095,0.05581,-0.11583,0.13083,0.03771,0.11093,-0.01351,-0.12047,0.09005,0.00524,-0.13015,-0.04959,-0.14188,0.06415,-0.08460,-0.09117,-0.12429,-0.13001,0.09449,0.02435,-0.03832,-0.07452,-0.16475,0.00541},
		[9] = {0.15727,-0.10273,0.24112,0.16127,-0.11781,-0.07718,0.00933,-0.41113,-0.10386,-0.08294,0.26641,-0.00704,-0.53730,-1.09816,-0.22921,-0.03163,-0.02884,0.14802,0.11862,0.10524,0.10292,-0.65560,0.08156,-0.07270,-0.34384,0.10157,-0.02418,0.08402,-0.44995,0.08742,0.10725,0.27048,-0.61234,0.19203,-0.10495,0.07231,-0.55499,-0.83409,0.08702,-0.21960,-0.17231,-0.13071,-0.39751,-0.00800,-0.46204,-0.01097,0.17840,0.10022},
		[10] = {-0.15711,0.02973,-0.21804,0.02780,-0.10316,-0.02707,0.07951,-0.01677,-0.12928,0.02205,0.14470,0.19711,-0.14928,0.00823,0.19732,0.13459,0.06282,0.23064,-0.15655,-0.07728,0.18597,-0.21478,-0.05338,-0.31695,0.11275,0.03888,0.01138,0.12041,0.31223,0.01766,0.30633,-0.06363,-0.05869,0.14328,0.09513,0.35390,-0.06077,-0.21019,-0.06583,-0.24336,0.15173,0.04604,0.23554,-0.17495,-0.28195,0.00304,-0.01611,0.02267},
		[11] = {-0.26359,0.11268,-0.91177,0.39825,-0.29832,0.08707,0.09347,0.49047,-0.03468,-0.04428,0.74752,-0.08369,0.06764,0.05301,-0.03823,0.01860,-0.34786,0.22424,0.23118,0.08647,-0.21273,0.35621,0.09348,-0.73729,0.11563,-0.79784,0.57486,-0.09066,0.51659,-0.07538,0.09944,-0.37130,0.00566,-0.09664,0.11173,0.13718,0.13249,0.50856,-0.13779,-0.28437,-0.39773,0.01908,0.16240,-0.32689,-0.28739,0.15988,-0.20418,0.19793},
		[12] = {0.01266,-0.00488,0.38951,0.00144,-0.52146,-0.05841,0.11128,-0.25620,-0.15221,-0.05458,-0.35205,0.15710,0.15237,0.58211,0.47139,0.42691,0.18779,0.44570,-0.18336,-0.02434,-0.23598,-0.58411,-0.11671,0.16832,-0.44528,-0.08641,-0.02276,-0.04077,0.02130,0.12608,-0.07310,0.32811,0.03320,0.07977,-0.10239,0.48974,0.40571,-0.22537,-0.14206,-0.75124,0.10564,0.00843,-0.20421,0.42527,-0.21929,0.14148,0.02022,-0.17618},
		[13] = {0.35388,-0.01953,-1.53019,0.50961,-0.02868,0.11938,-0.17019,-0.35564,-0.07810,0.07921,0.48640,0.11022,-0.26202,-0.50990,-0.19905,0.28461,0.65252,0.36239,-1.13895,0.03194,-0.29688,0.42775,-0.01684,-0.89282,0.68605,0.11635,0.40827,-0.05880,0.89382,-0.03533,0.25928,0.03054,-1.15013,0.24034,0.02371,-0.58920,-0.01607,-0.77328,-0.12396,0.02975,-0.74949,-0.05883,0.45018,-0.31362,-0.03515,0.63622,-0.02599,1.43469},
		[14] = {-0.27580,0.12028,0.07618,-0.13242,0.11248,-0.01654,-0.01002,0.15806,-0.10918,-0.12002,-0.80643,0.10212,0.14979,0.34555,0.04830,0.38174,-0.18009,-0.15068,-0.22793,-0.08952,0.11413,-0.02367,-0.05374,0.47355,0.00275,-0.58015,-0.64375,0.09547,-0.69756,0.08768,-0.09731,-0.35646,0.01344,-0.02284,-0.00583,-0.28004,0.27952,0.01129,0.01197,-0.08870,0.24987,0.04524,-0.30272,0.50872,-0.00342,-0.59896,0.86666,-0.28526},
		[15] = {0.04585,0.09477,-0.98262,0.29184,-0.13520,-0.00984,0.09086,-0.12164,0.02736,-0.12683,-0.34975,-0.05765,0.16225,-0.32601,0.10365,0.74708,0.40979,-0.15493,0.54452,0.11470,0.17789,-0.04314,0.01193,-0.46610,-0.08529,-0.13944,0.82058,-0.06021,0.82905,-0.01972,0.00285,0.16727,-0.37818,0.27485,-0.08380,-0.19970,0.23533,-0.51426,0.04920,-0.37759,-0.02508,-0.10604,-0.33872,0.32552,-0.01888,0.14275,0.31791,0.67404},
		[16] = {-0.57150,0.05356,-0.12166,-0.75451,0.32345,-0.13271,-0.04610,-0.20841,-0.09896,0.12094,-0.41132,-0.12878,-0.57639,0.26855,-0.20859,-0.45231,-0.01232,0.04183,-0.90016,0.05365,0.55130,0.21804,-0.03019,-0.37039,-0.39790,-1.57114,-0.54887,-0.08160,0.04771,0.14320,0.98318,-0.37296,-0.06453,-0.06450,0.07763,0.44555,0.16571,-0.10345,-0.13568,0.26525,-0.07388,0.01822,-0.40863,-0.32240,0.13804,0.65558,-0.70322,0.55330},
		[17] = {0.00935,0.11107,0.01838,0.10452,-0.16288,-0.02200,0.04313,-0.03054,-0.10225,-0.05778,-0.03029,-0.04900,-0.10389,-0.14576,-0.13668,-0.09115,-0.04552,-0.14855,-0.04227,-0.05366,0.04974,-0.11020,0.03622,0.00257,0.05335,0.03151,-0.07302,0.02218,-0.04248,0.10346,-0.14230,-0.00599,0.08884,0.14212,0.03658,-0.11378,-0.11034,0.03487,-0.05880,0.10372,0.07013,0.03307,0.04389,0.14634,-0.11083,-0.06819,-0.09070,-0.28334},
		[18] = {0.17939,-0.05802,0.51927,-0.12678,-0.02429,0.13248,-0.09770,-0.15377,0.09670,0.08689,0.06708,-0.06845,0.35961,0.16399,-0.18904,-0.18530,-0.07364,-0.10793,0.48374,0.07583,-0.35340,-0.59621,0.01282,0.17294,-0.20302,0.15686,0.11265,0.00062,-0.19749,0.06016,-0.77414,0.05152,0.04387,-0.05761,-0.12903,0.21856,-0.17655,0.37433,-0.10310,0.17358,-0.25508,0.00135,-0.35757,-0.17681,0.09207,-0.09626,-0.60452,-0.08040},
		[19] = {0.33843,-0.11298,-0.01654,-0.74490,0.53926,0.00006,-0.02167,-0.13136,-0.07258,0.12316,0.11410,-0.04309,-0.44171,1.15936,-0.13916,0.24437,0.01573,0.12069,0.20030,0.13241,-0.96808,-0.27014,-0.01425,-0.34923,-0.07315,0.45720,-0.02958,0.08410,0.01546,0.07326,0.37134,0.32880,-0.13931,-0.12629,0.04466,-0.18221,0.62468,0.08761,0.12294,-0.25546,0.13892,0.03876,-0.26353,-0.11892,-0.10955,-0.14838,-0.47302,0.07799},
		[20] = {-0.21005,0.04057,0.16978,0.19448,0.15713,0.11651,0.02379,-0.16429,-0.09494,0.12090,0.05226,0.11421,-0.64662,-1.02680,0.62609,-0.58405,0.34756,-0.19984,-0.40709,0.09476,0.68244,0.23691,-0.06735,-0.37485,0.68389,-0.97316,-1.19639,0.03267,-0.56791,-0.08836,0.86219,-0.38666,0.16195,0.26478,0.02720,-0.05298,-0.84401,-0.33015,-0.07630,-0.35014,0.54276,0.00735,1.04655,-0.32428,0.03219,-0.01357,0.25308,0.01696},
		[21] = {-1.29075,0.01834,0.42459,-0.06312,-0.06407,-0.02754,0.12032,-0.03154,-0.08768,0.08566,-0.59518,0.06989,-0.11631,0.38206,0.00940,0.02118,0.37815,0.00149,-0.30290,-0.11501,0.49374,-0.30099,-0.05974,0.05028,0.21598,-0.38362,-0.19368,0.06194,0.02176,-0.13356,0.29545,0.12136,-0.12744,-0.43534,-0.06184,1.00797,0.30783,-0.25788,-0.15450,-0.74429,0.12260,-0.14280,0.69085,0.52884,0.01761,0.41306,0.22114,-0.85887},
		[22] = {0.51721,0.04973,-0.00973,0.58073,0.29332,-0.05257,-0.13019,-0.04301,0.07844,0.03789,-0.04305,-0.20381,0.18464,-1.93297,-0.37937,-0.18436,0.13744,0.65199,-0.03841,-0.06150,-0.21232,0.33497,0.02051,-0.33904,-0.31660,0.08650,-0.09647,0.00680,-0.49426,0.01565,-0.57823,0.15936,-0.18805,-0.28326,-0.12342,-0.48791,-0.50991,1.69401,0.05033,0.66510,0.09241,0.02133,-0.18823,0.46997,0.86581,0.06913,-0.25133,-0.79261},
		[23] = {-0.06519,-0.05670,-0.12090,-0.06995,0.08702,-0.14017,0.06431,-0.04196,-0.12531,0.13633,0.03934,0.19889,-0.45697,0.29107,0.03049,0.16871,-0.02192,0.27508,-0.27362,0.02055,0.10906,-0.58820,-0.04479,-0.10746,-0.08205,-0.07592,-0.26733,-0.03416,0.06139,-0.11790,0.25580,0.32253,-0.00380,0.16246,-0.02413,0.06700,0.29573,0.04324,0.12796,-0.45182,0.23268,0.12338,0.17472,0.07150,-0.19592,-0.04126,0.37036,-0.35649},
		[24] = {-0.29671,0.08857,0.10120,-0.11936,-0.43682,0.10919,0.10388,-0.01601,-0.13269,-0.09652,0.17905,0.09916,0.02307,0.10053,0.09433,0.21334,0.21040,0.25906,0.07611,-0.12189,0.00039,-0.12336,0.10139,-0.12222,-0.04345,0.11268,0.25706,0.12455,0.21284,-0.03922,-0.09074,0.04444,0.00512,0.14337,-0.05006,0.36518,0.07694,-0.20537,0.01696,-0.29896,-0.05481,0.05966,0.22383,0.06610,-0.17059,-0.02508,0.28620,-0.09371},
		[25] = {-0.01671,-0.14218,0.76802,0.54437,-1.14017,-0.04444,-0.05378,0.32487,-0.03524,-0.00455,-0.29018,-0.16482,0.00056,-0.04260,0.59550,0.20695,0.88984,1.27970,0.35054,-0.14670,-0.20359,-0.20296,-0.10435,0.04378,-0.00903,-0.14547,0.01611,-0.01209,-0.19401,0.09258,0.49180,0.37638,0.09807,0.08944,0.07162,0.75927,-0.16829,-0.60789,-0.01635,-0.03579,-0.02614,0.00493,0.49512,0.68967,0.09652,0.29133,-0.18872,-1.29542},
		[26] = {-0.10742,-0.01050,-0.31411,0.07072,0.10903,0.13782,-0.08866,-0.03713,0.04287,0.04461,-0.17336,0.07022,-0.29091,-0.06903,0.06038,-0.13613,-0.09237,0.08228,-0.00257,-0.07042,0.23067,-0.09843,-0.03983,-0.42474,-0.15117,-0.01313,-0.49760,-0.01539,-0.26942,0.12175,-0.08641,-0.05381,0.07725,0.03914,0.05822,-0.03937,-0.22754,-0.09565,-0.04218,0.05057,0.23354,-0.09786,-0.12569,0.10291,-0.02695,0.07167,0.09570,0.12450},
		[27] = {0.05295,0.12954,-0.06679,0.03037,-0.01503,-0.05732,0.12402,0.15765,-0.10779,0.00160,-0.10674,0.12170,0.04144,0.08520,0.03402,0.14424,0.02445,-0.03353,-0.05139,0.04665,-0.00235,-0.43613,-0.00763,0.07679,0.02199,0.26978,0.00670,-0.11720,0.00424,-0.00760,-0.06351,0.19211,-0.13130,0.09915,0.09446,0.37666,0.28785,-0.05051,-0.03706,-0.23124,0.18609,0.01079,0.47740,0.17130,-0.48446,-0.01192,0.11046,-0.29641},
		[28] = {0.20730,0.07914,-0.13008,-0.05670,0.14434,0.03099,0.06757,-0.07700,-0.08104,-0.07893,0.08289,-0.04410,-0.15198,0.09377,-0.16909,-0.01715,0.09546,0.09963,-0.06933,-0.12893,-0.08514,-0.13743,-0.03502,-0.12787,-0.00009,0.11776,-0.06437,-0.02337,-0.09201,0.00810,-0.15347,0.09379,-0.19656,0.08246,0.10389,0.13184,-0.10648,0.05717,-0.08562,0.05568,-0.11778,-0.03273,-0.00277,0.07712,-0.02938,0.02412,-0.10932,-0.18229},
		[29] = {0.38026,0.12777,-1.06635,-0.76734,0.81879,-0.00363,0.01590,-0.47288,-0.12467,0.05143,-1.16819,-0.09371,-0.20751,-0.01576,0.37290,-0.76484,-0.13680,-0.72896,-0.49513,0.04394,-0.27685,0.32179,0.05617,-0.51192,0.61801,-0.09378,-1.77572,-0.07488,-0.11409,-0.07459,0.44599,-0.10366,-0.11288,-0.49751,-0.09256,0.05652,-0.26158,-0.00805,0.07818,-0.05965,0.36459,-0.11661,0.58497,0.43008,-0.02066,0.32995,-0.40919,-0.41164},
		[30] = {-0.03245,0.09618,-0.02865,-0.09078,0.09292,-0.05234,-0.05343,0.02291,-0.00844,-0.05534,0.06935,-0.13149,-0.05258,-0.05166,0.06131,0.04311,-0.13156,0.04746,0.03770,-0.04689,-0.21878,-0.14203,0.09993,-0.15695,-0.03449,-0.11680,0.05432,0.03256,0.04098,0.06333,0.06442,0.00885,-0.10924,-0.06622,0.11211,0.19293,0.06905,0.07226,0.09391,-0.11136,-0.01300,-0.02354,0.17400,0.14188,0.07292,0.00309,0.03838,0.02577},
		[31] = {-0.03185,0.08283,0.02379,0.01974,-0.13273,-0.05464,0.00849,-0.10933,0.01044,0.05087,0.07904,-0.12572,-0.09852,-0.09591,-0.10432,0.12049,0.03786,0.04817,0.13129,0.10601,-0.13486,0.04826,-0.05812,-0.07769,-0.02459,-0.00457,-0.14647,-0.11803,0.01663,0.02802,0.11215,-0.10704,0.06487,-0.12822,0.03526,-0.06331,-0.11552,0.11056,0.12475,0.06334,-0.10962,-0.09109,-0.04670,0.12088,-0.07148,0.07364,-0.09629,-0.08494},
		[32] = {-0.20040,-0.00279,0.02406,-0.01743,0.10679,-0.02113,-0.10115,0.02485,0.06665,-0.01013,-0.00998,-0.07122,0.00457,-0.07823,0.12428,-0.04086,0.18234,-0.05726,0.08293,-0.13951,-0.00551,-0.02476,0.03843,-0.02782,0.01193,-0.18636,0.07426,-0.01351,0.06080,0.01974,-0.04201,-0.06811,0.11364,-0.12236,0.00419,0.15027,-0.14959,-0.01654,-0.08815,0.13335,-0.07557,-0.09367,0.05700,-0.05371,0.11622,0.12097,0.00785,-0.18899},
	},
	output = {-1.28093,1.50881,0.23502,0.77035,0.80879,-0.19337,-1.97226,-0.16314,1.40016,0.21163,1.03993,0.90387,1.44338,-1.16536,-1.04485,-2.18261,0.33494,-1.34778,-1.28779,-2.03355,0.66418,-2.41992,0.65895,-0.34751,1.14775,0.79089,0.33614,0.44974,2.27646,-0.27749,0.08849,0.17873},
	biases = {
		hidden1 = {0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000},
		hidden2 = {0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000},
		output = 0.47185
	}
}  
-- KONFIGURACJA
local inputSize = 12
local h1_size = 48
local h2_size = 32
local m_clamp = math.clamp
local m_exp = math.exp
local m_abs = math.abs
local m_random = math.random
local m_sqrt = math.sqrt
-- MATEMATYKA 

local function activateDeriv(x) return x > 0 and 1 or 0.01 end
local function ensureNumericWeights()
	local W = AttentionNN.Weights
	if not W then return end

	for i = 1, h1_size do
		local key = "n"..i
		if W.hidden1[key] then
			W.hidden1[i] = W.hidden1[key]
			W.hidden1[key] = nil
		end
	end

	for i = 1, h2_size do
		local key = "n"..i
		if W.hidden2[key] then
			W.hidden2[i] = W.hidden2[key]
			W.hidden2[key] = nil
		end
	end
end

function AttentionNN.initWeights()
	local rng = Random.new()
	local function xavier(inSize) 
		return (rng:NextNumber() * 2 - 1) * math.sqrt(1 / inSize) 
	end

	AttentionNN.Weights = {
		hidden1 = {}, 
		hidden2 = {}, 
		output = {}, 
		biases = {hidden1 = {}, hidden2 = {}, output = 0}
	}

	for n = 1, h1_size do
		AttentionNN.Weights.hidden1[n] = {} -- USUNIĘTO ["n"..n]
		for i = 1, inputSize do 
			AttentionNN.Weights.hidden1[n][i] = xavier(inputSize) 
		end
		AttentionNN.Weights.biases.hidden1[n] = 0
	end

	for n = 1, h2_size do
		AttentionNN.Weights.hidden2[n] = {} -- USUNIĘTO ["n"..n]
		for i = 1, h1_size do 
			AttentionNN.Weights.hidden2[n][i] = xavier(h1_size) 
		end
		AttentionNN.Weights.biases.hidden2[n] = 0
	end

	for n = 1, h2_size do 
		AttentionNN.Weights.output[n] = xavier(h2_size) 
	end
	AttentionNN.Weights.biases.output = 0
end

-- MATEMATYKA
local function activate(x) return x > 0 and x or x * 0.01 end
local function finalActivate(x) return 1 / (1 + m_exp(-m_clamp(x, -20, 20))) end

-- NORMALIZACJA
local function getVector(token, position, avgS)
	local len = #token
	local bytes = {string.byte(token, 1, len)}
	local sum = 0
	for _, b in ipairs(bytes) do sum += b end

	local c1 = bytes[1] or 0
	local c2 = bytes[2] or 0
	local c3 = bytes[3] or 0
	local clast = bytes[len] or 0

	return {
		m_clamp(len / 20, 0, 1),
		m_clamp(sum / 1500, 0, 1),
		c1 / 255,
		c2 / 255,
		c3 / 255,
		clast / 255,
		(string.match(token, "%d") and 1 or 0),
		(string.match(token, "^%u") and 1 or 0),
		m_clamp(position / 10, 0, 1),
		(len % 2 == 0 and 1 or 0),
		m_clamp(sum % 100 / 100, 0, 1),
		m_clamp(sum / (avgS * 2), 0, 1)
	}
end

function AttentionNN.Forward(vec)
	ensureNumericWeights() -- Automatyczna naprawa przed każdym użyciem

	local l1, l2 = {}, {}
	local W = AttentionNN.Weights

	-- Warstwa Ukryta 1
	for n = 1, h1_size do
		local s = W.biases.hidden1[n] or 0
		local weights_n = W.hidden1[n]

		if weights_n then
			for i = 1, #vec do 
				s += vec[i] * (weights_n[i] or 0)
			end
		end
		l1[n] = activate(s)
	end

	-- Warstwa Ukryta 2
	for n = 1, h2_size do
		local s = W.biases.hidden2[n] or 0
		local weights_n = W.hidden2[n]

		if weights_n then
			for i = 1, h1_size do 
				s += l1[i] * (weights_n[i] or 0)
			end
		end
		l2[n] = activate(s)
	end

	-- Wyjście
	local outS = W.biases.output or 0
	for n = 1, h2_size do 
		outS += l2[n] * (W.output[n] or 0)
	end

	return finalActivate(outS), l1, l2
end





local l1_buffer = table.create(h1_size, 0)
local l2_buffer = table.create(h2_size, 0)
local dl2_buffer = table.create(h2_size, 0)

local function optimizeWeights(w)
	local new = {hidden1 = {}, hidden2 = {}, output = w.output, biases = w.biases}
	for i=1, h1_size do new.hidden1[i] = w.hidden1["n"..i] or w.hidden1[i] end
	for i=1, h2_size do new.hidden2[i] = w.hidden2["n"..i] or w.hidden2[i] end
	return new
end

function AttentionNN.Train(TrainingData, keepWeights)
	if not AttentionNN.Weights or not keepWeights then 
		AttentionNN.initWeights() 
	end

	-- Optymalizacja wag na format indeksowany numerycznie
	AttentionNN.Weights = optimizeWeights(AttentionNN.Weights)
	local W = AttentionNN.Weights
	local h1_w, h2_w, out_w = W.hidden1, W.hidden2, W.output
	local b1, b2, b_out = W.biases.hidden1, W.biases.hidden2, W.biases.output

	-- Pre-kalkulacja wejściowa
	local tdCount = #TrainingData
	local globalSum = 0
	for i = 1, tdCount do 
		local str = TrainingData[i][1]
		for j = 1, #str do globalSum += string.byte(str, j) end 
	end
	local avgS = globalSum / tdCount

	local preparedData = table.create(tdCount)
	for i = 1, tdCount do
		preparedData[i] = {vec = getVector(TrainingData[i][1], 1, avgS), target = TrainingData[i][2]}
	end

	-- Hiperparametry
	local lr = 0.03
	local min_lr, momentum = 0.008, 0.8
	local epochs = 6000

	-- Zmienne do kontroli czasu (Zapobieganie timeoutowi)
	local startTime = os.clock()
	local FRAME_BUDGET = 1/60 -- 16ms na klatkę

	-- Alokacja Velocity (Momentum)
	local v_h1, v_h2 = table.create(h1_size), table.create(h2_size)
	local v_b1, v_b2 = table.create(h1_size, 0), table.create(h2_size, 0)
	for i = 1, h1_size do v_h1[i] = table.create(inputSize, 0) end
	for i = 1, h2_size do v_h2[i] = table.create(h1_size, 0) end
	local v_out = table.create(h2_size, 0)
	local v_b_out = 0

	-- Buffery aktywacji
	local l1_buf = table.create(h1_size, 0)
	local l2_buf = table.create(h2_size, 0)
	local d_l2_buf = table.create(h2_size, 0)

	local lastError = 100
	local patience = 0

	print("Start treningu (Timeout Protection Active)...")

	for epoch = 1, epochs do
		local totalError = 0

		if os.clock() - startTime > FRAME_BUDGET then
			task.wait() -- Pozwól silnikowi Roblox odetchnąć
			startTime = os.clock()
		end

		-- Tasowanie danych
		for i = tdCount, 2, -1 do
			local j = m_random(i)
			preparedData[i], preparedData[j] = preparedData[j], preparedData[i]
		end

		for d_idx = 1, tdCount do
			local data = preparedData[d_idx]
			local vec, target = data.vec, data.target

			-- FORWARD
			for n = 1, h1_size do
				local s = b1[n]
				local row = h1_w[n]
				for i = 1, inputSize do s += vec[i] * row[i] end
				l1_buf[n] = s > 0 and s or s * 0.01
			end

			for n = 1, h2_size do
				local s = b2[n]
				local row = h2_w[n]
				for i = 1, h1_size do s += l1_buf[i] * row[i] end
				l2_buf[n] = s > 0 and s or s * 0.01
			end

			local outS = b_out
			for n = 1, h2_size do outS += l2_buf[n] * out_w[n] end
			local out = 1 / (1 + m_exp(-m_clamp(outS, -20, 20)))

			-- BACKPROPAGATION
			local err = target - out
			totalError += m_abs(err)
			local d_out = m_clamp(err * (out * (1 - out)), -0.2, 0.2)

			-- Update Output
			v_b_out = (v_b_out * momentum) + (d_out * lr)
			W.biases.output += v_b_out

			for n = 1, h2_size do
				local l2_val = l2_buf[n]
				v_out[n] = (v_out[n] * momentum) + (l2_val * d_out * lr)
				out_w[n] += v_out[n]

				local grad_h2 = d_out * out_w[n] * (l2_val > 0 and 1 or 0.01)
				d_l2_buf[n] = grad_h2

				local v_row = v_h2[n]
				local w_row = h2_w[n]
				for i = 1, h1_size do
					local v = (v_row[i] * momentum) + (l1_buf[i] * grad_h2 * lr)
					v_row[i] = v
					w_row[i] = m_clamp(w_row[i] + v, -3, 3)
				end
				
				v_b2[n] = (v_b2[n] * momentum) + (grad_h2 * lr)
				b2[n] = b2[n] + v_b2[n]


				
			end

			-- Update Hidden 1
			for n = 1, h1_size do
				local d_sum1 = 0
				for j = 1, h2_size do d_sum1 += d_l2_buf[j] * h2_w[j][n] end
				local d_l1 = d_sum1 * (l1_buf[n] > 0 and 1 or 0.01)

				local v_row = v_h1[n]
				local w_row = h1_w[n]
				for i = 1, inputSize do
					local v = (v_row[i] * momentum) + (vec[i] * d_l1 * lr)
					v_row[i] = v
					w_row[i] = m_clamp(w_row[i] + v, -3, 3)
				end
				v_b1[n] = (v_b1[n] * momentum) + (d_l1 * lr)
				b1[n] = b1[n] + v_b1[n]


			end
		end

		-- Logika uczenia 
		local currentErr = totalError / tdCount
		if currentErr >= lastError * 0.999 then
			patience += 1
			if patience > 10 then 
				lr = math.min(lr * 1.3, 0.05) 
				patience = 0 
			end
		else
			patience = 0
			lr = math.max(lr * 0.995, min_lr)
		end
		lastError = currentErr

		if epoch % 100 == 0 then
			print(string.format("Epoka: %d | Błąd: %.5f | LR: %.4f", epoch, currentErr, lr))
		end
	end
	print("Trening zakończony pomyślnie") 
end

-- PREDIKCJA
function AttentionNN.Predict(text)
	-- Dla predykcji przyjmujemy średnią similarity (0.5)
	local vec = getVector(text, 1, 100) 
	local out = AttentionNN.Forward(vec)
	return out
end

-- EKSPORT WAG, aktualnie nie potrzebne, tylko podczas treningu
function AttentionNN.ExportWeights()
	print("--- NOWY EKSPORT  ---")
	local function formatTable(t)
		if type(t) ~= "table" then return tostring(t) end
		local res = "{"
		for i = 1, #t do
			res = res .. string.format("%.5f", t[i]) .. (i < #t and "," or "")
		end
		return res .. "}"
	end

	local W = AttentionNN.Weights
	local s = "{\n"

	s = s .. "\thidden1 = {\n"
	for i = 1, h1_size do
		s = s .. "\t\t[" .. i .. "] = " .. formatTable(W.hidden1[i] or W.hidden1["n"..i]) .. ",\n"
	end

	s = s .. "\t},\n\thidden2 = {\n"
	for i = 1, h2_size do
		s = s .. "\t\t[" .. i .. "] = " .. formatTable(W.hidden2[i] or W.hidden2["n"..i]) .. ",\n"
	end

	s = s .. "\t},\n\toutput = " .. formatTable(W.output) .. ",\n"
	s = s .. "\tbiases = {\n"
	s = s .. "\t\thidden1 = " .. formatTable(W.biases.hidden1) .. ",\n"
	s = s .. "\t\thidden2 = " .. formatTable(W.biases.hidden2) .. ",\n"
	s = s .. "\t\toutput = " .. string.format("%.5f", W.biases.output) .. "\n\t}\n}"

	print(s)
end

return AttentionNN
